{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58892288/58889256 [==============================] - 218s 4us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "#Load the VGG model\n",
    "vgg_conv = VGG16(weights='imagenet', include_top=False, input_shape=(50, 50, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000002EB53C51978> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB54C47BA8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB600A61D0> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002EB600A67F0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB600D0C18> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB600F7630> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002EB6010C1D0> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB6011B550> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB60130D68> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB60141860> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002EB60162278> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB601735F8> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB60187E10> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB60199908> False\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002EB601BB320> False\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB601CA6A0> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB601DEF28> True\n",
      "<keras.layers.convolutional.Conv2D object at 0x000002EB601F19B0> True\n",
      "<keras.layers.pooling.MaxPooling2D object at 0x000002EB602153C8> True\n"
     ]
    }
   ],
   "source": [
    "# Freeze the layers except the last 4 layers\n",
    "for layer in vgg_conv.layers[:-4]:\n",
    "    layer.trainable = False\n",
    " \n",
    "# Check the trainable status of the individual layers\n",
    "for layer in vgg_conv.layers:\n",
    "    print(layer, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 15,242,050\n",
      "Trainable params: 7,606,786\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    " \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images belonging to 2 classes.\n",
      "Found 400 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "image_size = 50\n",
    "train_dir = './datasets/training_data'\n",
    "validation_dir = './datasets/testing_data'\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "# Change the batchsize according to your system RAM\n",
    "train_batchsize = 100\n",
    "val_batchsize = 10\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=train_batchsize,\n",
    "        class_mode='categorical')\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "10/10 [==============================] - 156s 16s/step - loss: 0.6992 - acc: 0.5490 - val_loss: 0.6220 - val_acc: 0.6425\n",
      "Epoch 2/30\n",
      "10/10 [==============================] - 151s 15s/step - loss: 0.6526 - acc: 0.6480 - val_loss: 0.5572 - val_acc: 0.7550\n",
      "Epoch 3/30\n",
      "10/10 [==============================] - 151s 15s/step - loss: 0.5726 - acc: 0.7060 - val_loss: 0.5777 - val_acc: 0.7000\n",
      "Epoch 4/30\n",
      "10/10 [==============================] - 150s 15s/step - loss: 0.5774 - acc: 0.6970 - val_loss: 0.5232 - val_acc: 0.7800\n",
      "Epoch 5/30\n",
      "10/10 [==============================] - 143s 14s/step - loss: 0.5430 - acc: 0.7390 - val_loss: 0.5100 - val_acc: 0.7625\n",
      "Epoch 6/30\n",
      "10/10 [==============================] - 144s 14s/step - loss: 0.5132 - acc: 0.7450 - val_loss: 0.4618 - val_acc: 0.7775\n",
      "Epoch 7/30\n",
      "10/10 [==============================] - 145s 14s/step - loss: 0.5208 - acc: 0.7640 - val_loss: 0.5073 - val_acc: 0.7425\n",
      "Epoch 8/30\n",
      " 4/10 [===========>..................] - ETA: 1:08 - loss: 0.4897 - acc: 0.7625"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "# Train the model\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=30,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    " \n",
    "# Save the model\n",
    "model.save('small_last4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    " \n",
    "plt.figure()\n",
    " \n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a generator for prediction\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=val_batchsize,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    " \n",
    "# Get the filenames from the generator\n",
    "fnames = validation_generator.filenames\n",
    " \n",
    "# Get the ground truth from generator\n",
    "ground_truth = validation_generator.classes\n",
    " \n",
    "# Get the label to class mapping from the generator\n",
    "label2index = validation_generator.class_indices\n",
    " \n",
    "# Getting the mapping from class index to class label\n",
    "idx2label = dict((v,k) for k,v in label2index.items())\n",
    " \n",
    "# Get the predictions from the model using the generator\n",
    "predictions = model.predict_generator(validation_generator, steps=validation_generator.samples/validation_generator.batch_size,verbose=1)\n",
    "predicted_classes = np.argmax(predictions,axis=1)\n",
    " \n",
    "errors = np.where(predicted_classes != ground_truth)[0]\n",
    "print(\"No of errors = {}/{}\".format(len(errors),validation_generator.samples))\n",
    " \n",
    "# Show the errors\n",
    "for i in range(len(errors)):\n",
    "    pred_class = np.argmax(predictions[errors[i]])\n",
    "    pred_label = idx2label[pred_class]\n",
    "     \n",
    "    title = 'Original label:{}, Prediction :{}, confidence : {:.3f}'.format(\n",
    "        fnames[errors[i]].split('/')[0],\n",
    "        pred_label,\n",
    "        predictions[errors[i]][pred_class])\n",
    "     \n",
    "    original = load_img('{}/{}'.format(validation_dir,fnames[errors[i]]))\n",
    "    plt.figure(figsize=[7,7])\n",
    "    plt.axis('off')\n",
    "    plt.title(title)\n",
    "    plt.imshow(original)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "severity_model = load_model('small_last4.h5')\n",
    "def prepare_img_256(img_path):\n",
    "    #urllib.request.urlretrieve(img_path, 'save.jpg') # or other way to upload image\n",
    "    img = load_img(img_path, target_size=(50, 50)) # this is a PIL image \n",
    "    x = img_to_array(img) # this is a Numpy array with shape (3, 256, 256)\n",
    "     \n",
    "    x = x.reshape((1,) + x.shape)/255\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "img_2561 = prepare_img_256('../datasets/Test Image/Test Folder/Red Swift with damage.jpg')\n",
    "#img_2562 = prepare_img_256('../datasets/Test Image/Test Folder/SwiftF2.jpg')\n",
    "#images = np.vstack([img_2561, img_2562])\n",
    "images = np.vstack([img_2561])\n",
    "\n",
    "plt.imshow(img_2561[0])\n",
    "plt.show()  \n",
    "#plt.imshow(img_2562[0])\n",
    "#plt.show()\n",
    "\n",
    "pred = severity_model.predict(images)[0][0]\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
